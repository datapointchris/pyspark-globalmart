{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Normalized Relational Table for Analysis in Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ETL Globalmart\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import boto3\n",
    "import pyspark\n",
    "from glob import glob\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMartDB:\n",
    "    \n",
    "    def __init__(self, connection_params):\n",
    "        self.connection_params = connection_params\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "        try:\n",
    "            connection = psycopg2.connect(**self.connection_params)\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            print(error)\n",
    "        print(\"Connection successful\")\n",
    "        self.connection = connection\n",
    "        return self\n",
    "    \n",
    "    def execute(self, sql):\n",
    "        with self.connection as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(sql)\n",
    "            results = cursor.fetchall()\n",
    "            return results\n",
    "        \n",
    "    def spark_df_builder(self, table, columns):\n",
    "        sql = f'select * from {table};'\n",
    "        results = self.execute(sql)\n",
    "        return spark.createDataFrame(data=results, schema=columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_df_builder(sql_statements):\n",
    "    dfs = []\n",
    "    for statement in sql_statements.values():\n",
    "        dfs.append(spark.sql(statement))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_reduce(df_list):\n",
    "    iterator = iter(df_list)\n",
    "    df = next(iterator)\n",
    "    for next_df in iterator:\n",
    "        df = df.join(next_df, on=['customer_id'], how='inner')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_parquet_files(bucket, prefix, parquet_dir):\n",
    "    for file in glob(f'{parquet_dir}/*'):\n",
    "        s3.upload_file(\n",
    "            Filename = file,\n",
    "            Bucket = bucket ,\n",
    "            Key = f'{prefix}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_connection = {\n",
    "    'host': 'globalmart.cb5gjaknyj4i.us-east-1.rds.amazonaws.com',\n",
    "    'database': 'globalmart',\n",
    "    'user': 'postgres',\n",
    "    'password': 'password'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDS Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {\n",
    "    'customers': [\n",
    "        \"customer_id\",\n",
    "        \"customer_email\",\n",
    "        \"customer_name\",\n",
    "        \"segment\",\n",
    "        \"country\",\n",
    "        \"city\",\n",
    "        \"state\",\n",
    "        \"postal_code\",\n",
    "        \"region\",\n",
    "    ],\n",
    "    \n",
    "    'vendors': [\"vendor_id\", \"vendor_name\"],\n",
    "    \n",
    "    'orders': [\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"vendor_id\",\n",
    "        \"ship_mode\",\n",
    "        \"order_status\",\n",
    "        \"order_purchase_date\",\n",
    "        \"order_approved_at\",\n",
    "        \"order_delivered_carrier_date\",\n",
    "        \"order_delivered_customer_date\",\n",
    "        \"order_estimated_delivery_date\",\n",
    "    ],\n",
    "    \n",
    "    'returns': [\"order_id\", \"return_reason\"],\n",
    "    \n",
    "    'transactions': [\n",
    "        \"transaction_id\",\n",
    "        \"order_id\",\n",
    "        \"ship_date\",\n",
    "        \"ship_mode\",\n",
    "        \"product_id\",\n",
    "        \"category\",\n",
    "        \"sub_category\",\n",
    "        \"product_name\",\n",
    "        \"sales\",\n",
    "        \"quantity\",\n",
    "        \"discount\",\n",
    "        \"profit\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql_statements = {\n",
    "    'total_orders': \"\"\"\n",
    "                    select customer_id,\n",
    "                    count(*) as orders\n",
    "                    from orders\n",
    "                    group by customer_id;\n",
    "                    \"\"\",\n",
    "    'total_spent': \"\"\"\n",
    "                    with order_totals as(\n",
    "                    select order_id,\n",
    "                    sum(sales) as total\n",
    "                    from transactions\n",
    "                    group by order_id\n",
    "                    order by total desc\n",
    "                    )\n",
    "                    select customer_id,\n",
    "                    round(cast(sum(order_totals.total) as decimal(10,2)), 2) as grand_total\n",
    "                    from orders\n",
    "                    join order_totals on order_totals.order_id = orders.order_id\n",
    "                    group by customer_id;\n",
    "                    \"\"\",\n",
    "    'purchase_frequency': \"\"\"\n",
    "                    select customer_id,\n",
    "                    cast(extract(day from (max(order_purchase_date) - min(order_purchase_date)) / count(order_purchase_date)) as integer) as avg_purchase_freq_days\n",
    "                    from orders\n",
    "                    group by customer_id;\n",
    "                    \"\"\",\n",
    "    'total_returns': \"\"\"\n",
    "                    with rtns as(\n",
    "                    select order_id,\n",
    "                    count(*) as no_returns\n",
    "                    from returns\n",
    "                    group by order_id\n",
    "                    )\n",
    "                    select customer_id,\n",
    "                    sum(rtns.no_returns) as total_returns\n",
    "                    from orders\n",
    "                    join rtns on rtns.order_id = orders.order_id\n",
    "                    group by orders.customer_id;\n",
    "                    \"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "db = GlobalMartDB(rds_connection).connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dfs = {}\n",
    "for table, columns in tables.items():\n",
    "    # create a dict of df\n",
    "    table_dfs[table] = db.spark_df_builder(table, columns)\n",
    "    # make a view with the name of each of the tables\n",
    "    table_dfs[table].createOrReplaceTempView(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/29 15:40:50 WARN TaskSetManager: Stage 16 contains a task of very large size (3497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/08/29 15:40:50 WARN TaskSetManager: Stage 17 contains a task of very large size (10419 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 19:==================================================>  (189 + 11) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|grand_total|\n",
      "+-----------+-----------+\n",
      "|101247     |36214.51   |\n",
      "|107311     |36117.05   |\n",
      "|111063     |36046.41   |\n",
      "|130695     |35918.45   |\n",
      "|120879     |35918.45   |\n",
      "|126943     |35865.05   |\n",
      "|117127     |35865.05   |\n",
      "|113375     |34765.84   |\n",
      "|103559     |34765.84   |\n",
      "|123191     |34446.88   |\n",
      "|133007     |34446.88   |\n",
      "|112807     |33369.18   |\n",
      "|102991     |33369.18   |\n",
      "|122623     |33345.30   |\n",
      "|132439     |33345.30   |\n",
      "|106743     |33148.28   |\n",
      "|126375     |33112.04   |\n",
      "|116559     |33112.04   |\n",
      "|102119     |32875.21   |\n",
      "|105871     |32819.64   |\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "with order_totals as(\n",
    "select order_id,\n",
    "sum(sales) as total\n",
    "from transactions\n",
    "group by order_id\n",
    "order by total desc\n",
    ")\n",
    "\n",
    "select customer_id,\n",
    "round(cast(sum(order_totals.total) as decimal(10,2)), 2) as grand_total\n",
    "from orders\n",
    "join order_totals on order_totals.order_id = orders.order_id\n",
    "group by customer_id\n",
    "order by grand_total desc;\n",
    "''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate and View Final Fact Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dfs = aggregated_df_builder(sql_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fact_table = spark_reduce(agg_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/29 16:03:41 WARN TaskSetManager: Stage 44 contains a task of very large size (3497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/08/29 16:03:41 WARN TaskSetManager: Stage 47 contains a task of very large size (3497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/08/29 16:03:44 WARN TaskSetManager: Stage 48 contains a task of very large size (3497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/08/29 16:03:44 WARN TaskSetManager: Stage 49 contains a task of very large size (10419 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 50:===============================================>     (181 + 12) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----------+----------------------+-------------+\n",
      "|customer_id|orders|grand_total|avg_purchase_freq_days|total_returns|\n",
      "+-----------+------+-----------+----------------------+-------------+\n",
      "|     100031|    16|    6391.12|                    27|            1|\n",
      "|     100501|    16|   11838.55|                    35|            1|\n",
      "|     100565|    16|    9052.58|                    32|            1|\n",
      "|     100847|    16|    6209.11|                    27|            1|\n",
      "|     100852|    16|    5033.11|                    29|            1|\n",
      "|     101011|    16|    7020.04|                    32|            1|\n",
      "|     101067|    16|    5588.62|                    31|            1|\n",
      "|     101155|    16|    6663.44|                    27|            1|\n",
      "|     101221|    16|    5278.70|                    34|            1|\n",
      "|     101333|    16|    8461.47|                    35|            1|\n",
      "|     101496|    16|    7124.85|                    34|            1|\n",
      "|     101507|    16|    5063.29|                    35|            1|\n",
      "|     101519|    16|    9304.02|                    24|            1|\n",
      "|     101552|    16|    6391.19|                    33|            1|\n",
      "|     101583|    16|    3343.38|                    25|            1|\n",
      "|     101738|    16|    3392.65|                    34|            1|\n",
      "|     101756|    16|    5033.20|                    31|            1|\n",
      "|     102092|    16|    8193.14|                    30|            1|\n",
      "|     102100|    16|   12256.40|                    28|            1|\n",
      "|     102454|    16|    4445.28|                    34|            1|\n",
      "+-----------+------+-----------+----------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 50:====================================================> (193 + 7) / 200]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fact_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- orders: long (nullable = false)\n",
      " |-- grand_total: decimal(10,2) (nullable = true)\n",
      " |-- avg_purchase_freq_days: integer (nullable = true)\n",
      " |-- total_returns: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe as Parquet and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/29 16:04:03 WARN TaskSetManager: Stage 52 contains a task of very large size (3497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/08/29 16:04:04 WARN TaskSetManager: Stage 55 contains a task of very large size (3497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/08/29 16:04:07 WARN TaskSetManager: Stage 56 contains a task of very large size (3497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/08/29 16:04:07 WARN TaskSetManager: Stage 57 contains a task of very large size (10419 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fact_table.write.mode('overwrite').parquet('customer_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', aws_access_key_id='',\n",
    "                      aws_secret_access_key='')\n",
    "bucket = 'on-premise-dump-files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_parquet_files(bucket, prefix='transformed/parquet', parquet_dir='customer_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
